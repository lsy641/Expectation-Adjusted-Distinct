{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO180aV28OAiFebuD/NYE9z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsy641/Expectation-Adjusted-Distinct/blob/main/EAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lseIhMzJrUbj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a651b97a-3310-48e6-c091-a9c9f3731fff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "!pip install transformers\n",
        "import transformers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random"
      ],
      "metadata": {
        "id": "o4zk4CNFDbIc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DyTwt3tKEdsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A guideline of using EAD\n",
        "\n",
        "## Formula\n",
        "\n",
        "$EAD = \\frac{N}{V(1-(\\frac{V-1}{V})^{C})}$\n",
        "\n",
        "$N$ - The number of distinct tokens  \n",
        "$V$ - The vocabulary size  \n",
        "$C$ - The number of total tokens  \n",
        "\n",
        "## Steps\n",
        "1. Determine the vocabulary size  \n",
        "2. Validate EAD on the training set  \n",
        "3. Calculate EAD score on the test set  \n",
        "\n"
      ],
      "metadata": {
        "id": "1i1Q4Oz0rYF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Determine the vocabulary size\n",
        "\n",
        "### Rules of thumb \n",
        "1. It is an empiracal and posterior approach to determining a vocabulary size. It is not a guide of vocabulary creating.\n",
        "\n",
        "2. The posterior assumption here means, supporse you have known the methods that are going to be evaluated, and these methods may adopt different vocabularies, now we have to determine a united value of vocabulary size for comparing these methods by calculating EAD scores.\n",
        "\n",
        "\n",
        "### Criterion\n",
        "\n",
        "\n",
        "|Scenarios|Desicion|\n",
        "|-|-|\n",
        "|Only one method (e.g., a benchmark work) | Directly use the size of vocabulary adopted by this method|\n",
        "|Several methods which are all not built based on large-scale pretrained models| Apply a third-party tokenizer to build the vocabulary for the dataset, and use the size of this vocabulary.|\n",
        "|Several methods which are built based on large-scale pretrained models| Directly use the largest vocabulary size among all vocabularies of the methods|\n",
        "|Some methods built on pretrained models, some are not | We never consider this situation due to the infair comparison problem |"
      ],
      "metadata": {
        "id": "chBGMyOVt7Yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Validate EAD on the training set\n",
        "\n"
      ],
      "metadata": {
        "id": "YeC_Vv23_BEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we just tokenize texts by considering space as the deliminator\n",
        "def read_opensub():\n",
        "    f = open(\"./ost.txt\",\"r\",encoding=\"utf-8\")\n",
        "    reader = csv.reader(f)\n",
        "\n",
        "    vocabulary = set()\n",
        "    original_text_os = []\n",
        "\n",
        "    for line in f.readlines():\n",
        "        texts = line.split(\" \")\n",
        "        original_text_os.append(texts)\n",
        "\n",
        "    random.shuffle(original_text_os)\n",
        "    \n",
        "    # data is too large\n",
        "    for text in original_text_os[:1000000]:\n",
        "        vocabulary = vocabulary | set(text)\n",
        "\n",
        "    return original_text_os, vocabulary\n",
        "    "
      ],
      "metadata": {
        "id": "TvCLivC1Bzm5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_text_os, vocabulary = read_opensub()"
      ],
      "metadata": {
        "id": "dHaCJSC-rZ3i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "f447be03-55e4-4b45-dd04-d036ab5d5574"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-373494afce27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moriginal_text_os\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_opensub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-290dfc06b8a3>\u001b[0m in \u001b[0;36mread_opensub\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Here we just tokenize texts by considering space as the deliminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_opensub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./ost.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './ost.txt'"
          ]
        }
      ]
    }
  ]
}